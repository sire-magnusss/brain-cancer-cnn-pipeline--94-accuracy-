{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14363254,"sourceType":"datasetVersion","datasetId":9171802}],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/magnusmakgasane/animetagger?scriptVersionId=289558985\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# =========================================================\n# WD14 / WD1.4 ANIME TAGGER (pretrained) + Grad-CAM (Keras 3 fix)\n# =========================================================\n\n!pip -q install huggingface_hub opencv-python\n\nimport os, csv\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport keras\nfrom huggingface_hub import hf_hub_download\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# -----------------------------\n# 1) Choose model + image\n# -----------------------------\nREPO_ID = \"SmilingWolf/wd-v1-4-convnext-tagger-v2\"\nMODEL_DIR = \"/kaggle/working/wd14_model\"\nIMG_PATH  = \"/kaggle/input/sasuke-img/Sasuke_Part_1.webp\"\n\n# -----------------------------\n# 2) Download model files (SavedModel + tags CSV)\n# -----------------------------\nos.makedirs(MODEL_DIR, exist_ok=True)\nvar_dir = os.path.join(MODEL_DIR, \"variables\")\nos.makedirs(var_dir, exist_ok=True)\n\nfiles_root = [\"saved_model.pb\", \"keras_metadata.pb\", \"selected_tags.csv\"]\nfiles_vars = [\"variables.data-00000-of-00001\", \"variables.index\"]\n\ndef dl(repo_id, filename, local_path, subfolder=None):\n    path = hf_hub_download(repo_id=repo_id, filename=filename, subfolder=subfolder, local_dir=os.path.dirname(local_path))\n    if path != local_path:\n        os.replace(path, local_path)\n\nfor f in files_root:\n    dl(REPO_ID, f, os.path.join(MODEL_DIR, f))\nfor f in files_vars:\n    dl(REPO_ID, f, os.path.join(var_dir, f), subfolder=\"variables\")\n\nprint(\"Model files ready at:\", MODEL_DIR)\n\n# -----------------------------\n# 3) Read tags\n# -----------------------------\ntags = []\ncats = []\nwith open(os.path.join(MODEL_DIR, \"selected_tags.csv\"), \"r\", encoding=\"utf-8\") as f:\n    reader = csv.reader(f)\n    _ = next(reader)  # header\n    for row in reader:\n        tags.append(row[1])   # name\n        cats.append(row[2])   # category (0 general, 4 character)\n\n# -----------------------------\n# 4) Keras 3: Load SavedModel via TFSMLayer (auto-find endpoint)\n# -----------------------------\n# Find available endpoints inside the SavedModel\nloaded_sm = tf.saved_model.load(MODEL_DIR)\nendpoints = list(loaded_sm.signatures.keys())\nprint(\"Available endpoints:\", endpoints)\n\n# Pick the first (usually 'serving_default')\nendpoint = \"serving_default\" if \"serving_default\" in endpoints else endpoints[0]\nprint(\"Using endpoint:\", endpoint)\n\n# Wrap as a Keras model\nlayer = keras.layers.TFSMLayer(MODEL_DIR, call_endpoint=endpoint)\n\n# We need an input shape; WD14 tagger uses 448x448x3\nIMAGE_SIZE = 448\ninp = keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), dtype=tf.float32)\nout = layer(inp)\n# Some TFSMLayer outputs are dict-like; normalize to a tensor\nif isinstance(out, dict):\n    out = list(out.values())[0]\nmodel = keras.Model(inp, out)\n\n# -----------------------------\n# 5) Preprocess image (WD14 expects BGR, padded square, 448, float32 0..255)\n# -----------------------------\ndef preprocess_wd14(pil_img):\n    img = np.array(pil_img.convert(\"RGB\"))\n    img = img[:, :, ::-1]  # RGB -> BGR\n\n    size = max(img.shape[0], img.shape[1])\n    pad_x = size - img.shape[1]\n    pad_y = size - img.shape[0]\n    pad_l = pad_x // 2\n    pad_t = pad_y // 2\n    img = np.pad(img,\n                 ((pad_t, pad_y - pad_t), (pad_l, pad_x - pad_l), (0, 0)),\n                 mode=\"constant\", constant_values=255)\n\n    interp = cv2.INTER_AREA if size > IMAGE_SIZE else cv2.INTER_LANCZOS4\n    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=interp)\n    return img.astype(np.float32)\n\npil = Image.open(IMG_PATH)\nx_bgr = preprocess_wd14(pil)\nx = np.expand_dims(x_bgr, 0)\n\nplt.figure(figsize=(4,4))\nplt.imshow(x_bgr[:, :, ::-1].astype(np.uint8))\nplt.axis(\"off\")\nplt.title(\"WD14 input (square padded)\")\nplt.show()\n\n# -----------------------------\n# 6) Predict tags\n# -----------------------------\nprobs = model(x, training=False).numpy()[0]\n\n# WD14 models often have 4 rating outputs first\nstart = 4\n\nGENERAL_THRESH = 0.35\nCHAR_THRESH = 0.50\n\ngeneral, chars = [], []\nfor i in range(start, len(probs)):\n    p = float(probs[i])\n    name = tags[i].replace(\"_\", \" \")\n    cat = cats[i]\n    if cat == \"0\" and p >= GENERAL_THRESH:\n        general.append((name, p))\n    elif cat == \"4\" and p >= CHAR_THRESH:\n        chars.append((name, p))\n\ngeneral.sort(key=lambda t: t[1], reverse=True)\nchars.sort(key=lambda t: t[1], reverse=True)\n\nprint(\"=== Character tags ===\")\nfor name, p in chars[:15]:\n    print(f\"{name:30s} {p:.3f}\")\n\nprint(\"\\n=== General tags ===\")\nfor name, p in general[:20]:\n    print(f\"{name:30s} {p:.3f}\")\n\ntop_idx = int(np.argmax(probs[start:]) + start)\ntop_tag = tags[top_idx].replace(\"_\", \" \")\ntop_p = float(probs[top_idx])\nprint(f\"\\nTop tag for Grad-CAM: {top_tag}  (p={top_p:.3f})\")\n\n# -----------------------------\n# 7) Grad-CAM: find last conv-like 4D layer inside TFSMLayer output graph\n#    Since TFSMLayer is opaque, classic \"pick last conv layer by name\" won't work.\n#    We'll instead use Grad-CAM on the INPUT by computing gradients w.r.t input pixels.\n#    This is still a valid saliency-style heatmap and looks very AI-ish.\n# -----------------------------\nx_tf = tf.convert_to_tensor(x)\n\nwith tf.GradientTape() as tape:\n    tape.watch(x_tf)\n    y = model(x_tf, training=False)  # (1, num_labels)\n    score = y[:, top_idx]\n\ngrads = tape.gradient(score, x_tf)[0]  # (448,448,3)\ngrads = tf.reduce_mean(tf.abs(grads), axis=-1)  # (448,448)\nheatmap = grads.numpy()\nheatmap = heatmap / (heatmap.max() + 1e-8)\n\n# Overlay\nbase_rgb = x_bgr[:, :, ::-1].astype(np.uint8)\nheat_u8 = (heatmap * 255).astype(np.uint8)\nheat_color = cv2.applyColorMap(heat_u8, cv2.COLORMAP_JET)\nheat_color = cv2.cvtColor(heat_color, cv2.COLOR_BGR2RGB)\noverlay = cv2.addWeighted(base_rgb, 0.55, heat_color, 0.45, 0)\n\nplt.figure(figsize=(10,4))\nplt.subplot(1,3,1); plt.imshow(base_rgb); plt.axis(\"off\"); plt.title(\"Input to tagger\")\nplt.subplot(1,3,2); plt.imshow(heatmap, cmap=\"jet\"); plt.axis(\"off\"); plt.title(\"Saliency heatmap\")\nplt.subplot(1,3,3); plt.imshow(overlay); plt.axis(\"off\"); plt.title(f\"Overlay: {top_tag}\")\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:06:24.332252Z","iopub.execute_input":"2026-01-01T14:06:24.332965Z","iopub.status.idle":"2026-01-01T14:07:02.322394Z","shell.execute_reply.started":"2026-01-01T14:06:24.332917Z","shell.execute_reply":"2026-01-01T14:07:02.321588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Build a tidy table of top tags (excluding first 4 rating outputs)\nstart = 4\ndf = pd.DataFrame({\n    \"tag\": [t.replace(\"_\",\" \") for t in tags[start:]],\n    \"prob\": probs[start:],\n    \"cat\":  [c for c in cats[start:]]\n})\n\n# show top 25 overall\ntop = df.sort_values(\"prob\", ascending=False).head(25)\n\ndisplay(top)\n\nplt.figure(figsize=(8,6))\nplt.barh(top[\"tag\"][::-1], top[\"prob\"][::-1])\nplt.title(\"Top 25 tags (WD14)\")\nplt.xlabel(\"probability\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:12:20.245653Z","iopub.execute_input":"2026-01-01T14:12:20.246044Z","iopub.status.idle":"2026-01-01T14:12:20.501207Z","shell.execute_reply.started":"2026-01-01T14:12:20.246024Z","shell.execute_reply":"2026-01-01T14:12:20.500579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pick best character tag (cat == \"4\")\nstart = 4\nchar_indices = [i for i in range(start, len(probs)) if cats[i] == \"4\"]\nif len(char_indices) == 0:\n    print(\"No character tags above threshold found.\")\nelse:\n    target_index = max(char_indices, key=lambda i: probs[i])\n    target_tag = tags[target_index].replace(\"_\",\" \")\n    print(\"Heatmap target:\", target_tag, \"p=\", float(probs[target_index]))\n\n    x_tf = tf.convert_to_tensor(x)\n    with tf.GradientTape() as tape:\n        tape.watch(x_tf)\n        y = model(x_tf, training=False)\n        score = y[:, target_index]\n\n    grads = tape.gradient(score, x_tf)[0]\n    grads = tf.reduce_mean(tf.abs(grads), axis=-1)\n    heatmap = grads.numpy()\n    heatmap = heatmap / (heatmap.max() + 1e-8)\n\n    # smooth it (looks way nicer)\n    heatmap = cv2.GaussianBlur(heatmap, (0,0), sigmaX=2.0)\n    heatmap = heatmap / (heatmap.max() + 1e-8)\n\n    base_rgb = x_bgr[:, :, ::-1].astype(np.uint8)\n    heat_u8 = (heatmap * 255).astype(np.uint8)\n    heat_color = cv2.applyColorMap(heat_u8, cv2.COLORMAP_JET)\n    heat_color = cv2.cvtColor(heat_color, cv2.COLOR_BGR2RGB)\n    overlay = cv2.addWeighted(base_rgb, 0.55, heat_color, 0.45, 0)\n\n    plt.figure(figsize=(10,4))\n    plt.subplot(1,3,1); plt.imshow(base_rgb); plt.axis(\"off\"); plt.title(\"Input\")\n    plt.subplot(1,3,2); plt.imshow(heatmap, cmap=\"jet\"); plt.axis(\"off\"); plt.title(\"Smoothed heatmap\")\n    plt.subplot(1,3,3); plt.imshow(overlay); plt.axis(\"off\"); plt.title(f\"Overlay: {target_tag}\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:12:39.873602Z","iopub.execute_input":"2026-01-01T14:12:39.874057Z","iopub.status.idle":"2026-01-01T14:12:44.713658Z","shell.execute_reply.started":"2026-01-01T14:12:39.874035Z","shell.execute_reply":"2026-01-01T14:12:44.712932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, glob\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# folder with many images\nDATA_DIR = \"/kaggle/input/sasuke-img\"  # change to your anime dataset folder\n\nexts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.webp\",\"*.bmp\")\npaths = []\nfor e in exts:\n    paths.extend(glob.glob(os.path.join(DATA_DIR, \"**\", e), recursive=True))\n\npaths = sorted(paths)\nprint(\"Found images:\", len(paths))\n\ndef embed_one(path):\n    pil = Image.open(path)\n    xbgr = preprocess_wd14(pil)\n    xx = np.expand_dims(xbgr, 0)\n    p = model(xx, training=False).numpy()[0]\n    vec = p[4:]                      # tag probabilities as an \"embedding\"\n    vec = vec / (np.linalg.norm(vec) + 1e-8)\n    return vec\n\n# pick a query image (index 0 here)\nquery_path = paths[0]\nqvec = embed_one(query_path)\n\n# compute similarity\nsims = []\nfor pth in paths:\n    v = embed_one(pth)\n    sims.append((float(v @ qvec), pth))\n\nsims.sort(reverse=True, key=lambda t: t[0])\n\nprint(\"Query:\", query_path)\nfor s, pth in sims[:10]:\n    print(f\"{s:.3f}  {pth}\")\n\n# show top-6 similar\nplt.figure(figsize=(12,6))\nfor i, (_, pth) in enumerate(sims[:6]):\n    plt.subplot(2,3,i+1)\n    plt.imshow(Image.open(pth).convert(\"RGB\"))\n    plt.title(f\"sim={sims[i][0]:.3f}\")\n    plt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:12:58.650811Z","iopub.execute_input":"2026-01-01T14:12:58.651649Z","iopub.status.idle":"2026-01-01T14:13:01.095731Z","shell.execute_reply.started":"2026-01-01T14:12:58.651617Z","shell.execute_reply":"2026-01-01T14:13:01.094572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install transformers sentencepiece\n\nfrom transformers import pipeline\nfrom PIL import Image\n\nIMG_PATH = \"/kaggle/input/sasuke-img/Sasuke_Part_1.webp\"\n\ncaptioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\nout = captioner(Image.open(IMG_PATH).convert(\"RGB\"))\nprint(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:13:39.045421Z","iopub.execute_input":"2026-01-01T14:13:39.04573Z","iopub.status.idle":"2026-01-01T14:13:57.869646Z","shell.execute_reply.started":"2026-01-01T14:13:39.045713Z","shell.execute_reply":"2026-01-01T14:13:57.868626Z"}},"outputs":[],"execution_count":null}]}